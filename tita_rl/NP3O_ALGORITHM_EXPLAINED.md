# NP3O ç®—æ³•è¯´æ˜æ–‡æ¡£

## âœ… ç®€çŸ­å›ç­”

**æ˜¯çš„ï¼** è·‘é…·è®­ç»ƒä¾æ—§ä½¿ç”¨ **NP3O (Natural Policy Gradient with Proximal Policy Optimization)** ç®—æ³•ã€‚

è¿™æ˜¯åŸå§‹ `tita_constraint` è®­ç»ƒå°±åœ¨ä½¿ç”¨çš„ç®—æ³•ï¼Œè·‘é…·è®­ç»ƒ**å®Œå…¨ç»§æ‰¿**äº†è¿™ä¸ªç®—æ³•ï¼Œæ²¡æœ‰æ”¹å˜ã€‚

---

## 1ï¸âƒ£ ä»€ä¹ˆæ˜¯ NP3Oï¼Ÿ

### æ ¸å¿ƒå®šä¹‰

**NP3O** æ˜¯ä¸€ä¸ª**å¸¦çº¦æŸçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•**ï¼Œç»“åˆäº†ï¼š
- **PPO (Proximal Policy Optimization)**ï¼šç¨³å®šçš„ç­–ç•¥ä¼˜åŒ–
- **çº¦æŸä¼˜åŒ– (Constrained Optimization)**ï¼šç¡®ä¿æœºå™¨äººå®‰å…¨è¿åŠ¨

### é…ç½®ä¸­çš„ä½“ç°

```python
# configs/tita_parkour_config.py
class runner(TitaConstraintRoughCfgPPO.runner):
    policy_class_name = 'ActorCriticBarlowTwins'  # ç­–ç•¥ç½‘ç»œ
    runner_class_name = 'OnConstraintPolicyRunner'  # çº¦æŸè¿è¡Œå™¨
    algorithm_class_name = 'NP3O'  # âœ… NP3O ç®—æ³•
```

---

## 2ï¸âƒ£ NP3O vs PPO å¯¹æ¯”

### æ ¸å¿ƒå·®å¼‚

| ç‰¹æ€§ | æ ‡å‡† PPO | NP3O |
|------|---------|------|
| **ä¼˜åŒ–ç›®æ ‡** | æœ€å¤§åŒ–å¥–åŠ± | æœ€å¤§åŒ–å¥–åŠ± + æ»¡è¶³çº¦æŸ |
| **çº¦æŸå¤„ç†** | æ—  | æœ‰ï¼ˆé€šè¿‡ Cost Criticï¼‰ |
| **å®‰å…¨ä¿è¯** | æ— æ˜ç¡®ä¿è¯ | æœ‰è½¯çº¦æŸä¿è¯ |
| **é€‚ç”¨åœºæ™¯** | ä¸€èˆ¬ä»»åŠ¡ | æœºå™¨äººæ§åˆ¶ï¼ˆéœ€è¦å®‰å…¨æ€§ï¼‰ |

### æ•°å­¦è¡¨è¾¾

**æ ‡å‡† PPO**ï¼š
```
æœ€å¤§åŒ–: E[Î£ reward_t]
```

**NP3O**ï¼š
```
æœ€å¤§åŒ–: E[Î£ reward_t]
æ»¡è¶³çº¦æŸ: E[Î£ cost_t] â‰¤ d  (æœŸæœ›çº¦æŸå€¼)
```

---

## 3ï¸âƒ£ NP3O çš„å…³é”®ç»„ä»¶

### ç»„ä»¶ 1ï¼šåŒ Critic ç½‘ç»œ

```python
class ActorCritic:
    def __init__(self):
        self.actor = Actor()        # è¾“å‡ºåŠ¨ä½œ
        self.critic = Critic()      # è¯„ä¼°ä»·å€¼ï¼ˆå¥–åŠ±ï¼‰
        self.cost_critic = CostCritic()  # âœ… è¯„ä¼°ä»£ä»·ï¼ˆçº¦æŸï¼‰
```

**ä½œç”¨**ï¼š
- **Critic**ï¼šé¢„æµ‹"è¿™ä¸ªçŠ¶æ€èƒ½è·å¾—å¤šå°‘å¥–åŠ±"
- **Cost Critic**ï¼šé¢„æµ‹"è¿™ä¸ªçŠ¶æ€ä¼šè¿åå¤šå°‘çº¦æŸ"

### ç»„ä»¶ 2ï¼šçº¦æŸæŸå¤±å‡½æ•°

```python
# algorithm/np3o.py
class NP3O:
    def __init__(self, cost_value_loss_coef=1.0, cost_viol_loss_coef=1.0):
        self.cost_value_loss_coef = cost_value_loss_coef  # ä»£ä»·å€¼æŸå¤±ç³»æ•°
        self.cost_viol_loss_coef = cost_viol_loss_coef    # ä»£ä»·è¿åæŸå¤±ç³»æ•°
```

**é…ç½®**ï¼š
```python
# configs/tita_parkour_config.py
class algorithm:
    cost_value_loss_coef = 0.1  # ä»£ä»·å€¼æŸå¤±æƒé‡
    cost_viol_loss_coef = 0.1   # ä»£ä»·è¿åæŸå¤±æƒé‡
```

### ç»„ä»¶ 3ï¼šä»£ä»·ï¼ˆCostï¼‰å®šä¹‰

åœ¨ `tita_constraint_config.py` ä¸­å®šä¹‰äº† 6 ç§ä»£ä»·ï¼š

```python
class costs:
    class scales:
        pos_limit = 0.3            # å…³èŠ‚ä½ç½®é™åˆ¶
        torque_limit = 0.3         # åŠ›çŸ©é™åˆ¶
        dof_vel_limits = 0.3       # å…³èŠ‚é€Ÿåº¦é™åˆ¶
        acc_smoothness = 0.1       # åŠ é€Ÿåº¦å¹³æ»‘æ€§
        feet_contact_forces = 0.1  # è„šæ¥è§¦åŠ›
        stumble = 0.1              # ç»Šå€’æƒ©ç½š
    
    class d_values:  # æœŸæœ›çº¦æŸå€¼ï¼ˆç›®æ ‡ï¼‰
        pos_limit = 0.0
        torque_limit = 0.0
        dof_vel_limits = 0.0
        acc_smoothness = 0.0
        feet_contact_forces = 0.0
        stumble = 0.0

class cost:
    num_costs = 6  # æ€»å…± 6 ä¸ªçº¦æŸ
```

---

## 4ï¸âƒ£ NP3O çš„å·¥ä½œæµç¨‹

### è®­ç»ƒå¾ªç¯

```python
for iteration in range(max_iterations):
    # 1. æ”¶é›†æ•°æ®ï¼ˆRolloutï¼‰
    for step in range(num_steps_per_env):
        action = policy.act(obs)
        obs, reward, cost, done = env.step(action)  # âœ… è·å– reward å’Œ cost
        
        storage.add(obs, action, reward, cost)  # å­˜å‚¨å¥–åŠ±å’Œä»£ä»·
    
    # 2. è®¡ç®—ä¼˜åŠ¿å‡½æ•°ï¼ˆAdvantageï¼‰
    reward_advantages = compute_gae(rewards)  # åŸºäºå¥–åŠ±
    cost_advantages = compute_gae(costs)      # âœ… åŸºäºä»£ä»·
    
    # 3. æ›´æ–°ç­–ç•¥ï¼ˆNP3O ç‰¹æœ‰ï¼‰
    for epoch in range(num_learning_epochs):
        # ç­–ç•¥æŸå¤±ï¼ˆPPOï¼‰
        policy_loss = compute_ppo_loss(advantages=reward_advantages)
        
        # ä»·å€¼æŸå¤±ï¼ˆå¥–åŠ± Criticï¼‰
        value_loss = compute_value_loss(predicted_values, target_values)
        
        # âœ… ä»£ä»·ä»·å€¼æŸå¤±ï¼ˆCost Criticï¼‰
        cost_value_loss = compute_cost_value_loss(predicted_costs, target_costs)
        
        # âœ… ä»£ä»·è¿åæŸå¤±ï¼ˆæƒ©ç½šè¿åçº¦æŸï¼‰
        cost_viol_loss = compute_cost_violation_loss(costs, d_values)
        
        # æ€»æŸå¤±
        total_loss = (policy_loss + 
                      value_loss + 
                      cost_value_loss_coef * cost_value_loss +  # âœ… æ–°å¢
                      cost_viol_loss_coef * cost_viol_loss)     # âœ… æ–°å¢
        
        optimizer.step()
```

### å…³é”®å·®å¼‚

| æ­¥éª¤ | æ ‡å‡† PPO | NP3O |
|------|---------|------|
| ç¯å¢ƒåé¦ˆ | `reward` | `reward` + `cost` âœ… |
| ä¼˜åŠ¿è®¡ç®— | åªè®¡ç®—å¥–åŠ±ä¼˜åŠ¿ | å¥–åŠ±ä¼˜åŠ¿ + ä»£ä»·ä¼˜åŠ¿ âœ… |
| æŸå¤±å‡½æ•° | ç­–ç•¥æŸå¤± + ä»·å€¼æŸå¤± | + ä»£ä»·ä»·å€¼æŸå¤± + è¿åæŸå¤± âœ… |
| ç½‘ç»œç»“æ„ | Actor + Critic | Actor + Critic + Cost Critic âœ… |

---

## 5ï¸âƒ£ ä¸ºä»€ä¹ˆè·‘é…·ç”¨ NP3Oï¼Ÿ

### ä¼˜åŠ¿ 1ï¼šå®‰å…¨æ€§ä¿è¯

**åœºæ™¯**ï¼šæœºå™¨äººåœ¨é«˜é€Ÿè·‘é…·æ—¶å®¹æ˜“ï¼š
- å…³èŠ‚è¶…é™ï¼ˆæŸåç¡¬ä»¶ï¼‰
- åŠ›çŸ©è¿‡å¤§ï¼ˆç”µæœºçƒ§æ¯ï¼‰
- è¿åŠ¨è¿‡çŒ›ï¼ˆæ‘”å€’å—ä¼¤ï¼‰

**NP3O çš„ä½œç”¨**ï¼š
```python
# é€šè¿‡çº¦æŸé™åˆ¶è¿™äº›å±é™©è¡Œä¸º
cost_scales = {
    'pos_limit': 0.3,      # é™åˆ¶å…³èŠ‚è§’åº¦
    'torque_limit': 0.3,   # é™åˆ¶ç”µæœºåŠ›çŸ©
    'stumble': 0.1,        # é¿å…ç»Šå€’
}

# å¦‚æœè¿åçº¦æŸï¼ŒCost Critic ä¼šé¢„æµ‹é«˜ä»£ä»·
# ç­–ç•¥ä¼šå­¦ä¹ é¿å…è¿™äº›é«˜ä»£ä»·çš„åŠ¨ä½œ
```

### ä¼˜åŠ¿ 2ï¼šå¤šç›®æ ‡å¹³è¡¡

**è·‘é…·çš„å¤šé‡ç›®æ ‡**ï¼š
1. è·Ÿéšé€Ÿåº¦å‘½ä»¤ï¼ˆå¥–åŠ±ï¼‰
2. æ¸…é™¤éšœç¢ï¼ˆå¥–åŠ±ï¼‰
3. é¿å…å…³èŠ‚è¶…é™ï¼ˆçº¦æŸï¼‰
4. é¿å…åŠ›çŸ©è¿‡å¤§ï¼ˆçº¦æŸï¼‰
5. ä¿æŒè¿åŠ¨å¹³æ»‘ï¼ˆçº¦æŸï¼‰

**NP3O çš„å¤„ç†**ï¼š
```python
# å¥–åŠ±ï¼šé¼“åŠ±å¥½çš„è¡Œä¸º
rewards = {
    'tracking_lin_vel': 1.0,
    'obstacle_clearance': 2.0,
    'jump_timing': 1.5,
}

# çº¦æŸï¼šé™åˆ¶å±é™©è¡Œä¸º
costs = {
    'pos_limit': 0.3,
    'torque_limit': 0.3,
    'acc_smoothness': 0.1,
}

# NP3O åŒæ—¶ä¼˜åŒ–å¥–åŠ±å’Œçº¦æŸ
# æ‰¾åˆ°"é«˜å¥–åŠ± + ä½ä»£ä»·"çš„æœ€ä¼˜ç­–ç•¥
```

### ä¼˜åŠ¿ 3ï¼šæ³›åŒ–åˆ°çœŸå®æœºå™¨äºº

**é—®é¢˜**ï¼šä»¿çœŸåˆ°ç°å®çš„è¿ç§»ï¼ˆSim2Realï¼‰

**æ ‡å‡† PPO**ï¼š
- å¯èƒ½å­¦åˆ°"ä½œå¼Š"åŠ¨ä½œï¼ˆä»¿çœŸæœ‰æ•ˆï¼Œç°å®å¤±è´¥ï¼‰
- ä¾‹å¦‚ï¼šä»¿çœŸä¸­å¯ä»¥ç¬é—´åŠ é€Ÿï¼Œç°å®ä¸­ç”µæœºè·Ÿä¸ä¸Š

**NP3O**ï¼š
- é€šè¿‡çº¦æŸé™åˆ¶ä¸åˆ‡å®é™…çš„åŠ¨ä½œ
- å­¦åˆ°çš„ç­–ç•¥æ›´æ¥è¿‘çœŸå®æœºå™¨äººçš„ç‰©ç†é™åˆ¶
- Sim2Real è¿ç§»æˆåŠŸç‡æ›´é«˜

---

## 6ï¸âƒ£ è·‘é…·è®­ç»ƒä¸­çš„ NP3O é…ç½®

### å®Œæ•´é…ç½®

```python
# configs/tita_parkour_config.py

class TitaParkourCfgPPO(TitaConstraintRoughCfgPPO):
    class algorithm(TitaConstraintRoughCfgPPO.algorithm):
        # PPO å‚æ•°
        entropy_coef = 0.01
        learning_rate = 1.e-3
        max_grad_norm = 0.01
        num_learning_epochs = 5
        num_mini_batches = 4
        
        # âœ… NP3O ç‰¹æœ‰å‚æ•°
        cost_value_loss_coef = 0.1   # ä»£ä»·ä»·å€¼æŸå¤±æƒé‡
        cost_viol_loss_coef = 0.1    # ä»£ä»·è¿åæŸå¤±æƒé‡
    
    class policy(TitaConstraintRoughCfgPPO.policy):
        # ç½‘ç»œç»“æ„
        actor_hidden_dims = [512, 256, 128]
        critic_hidden_dims = [512, 256, 128]
        
        # âœ… NP3O ç‰¹æœ‰
        num_costs = 6  # 6 ä¸ªçº¦æŸ
    
    class runner(TitaConstraintRoughCfgPPO.runner):
        # âœ… NP3O ç®—æ³•
        algorithm_class_name = 'NP3O'
        runner_class_name = 'OnConstraintPolicyRunner'
        policy_class_name = 'ActorCriticBarlowTwins'
```

### ç»§æ‰¿å…³ç³»

```
è·‘é…·é…ç½® ç»§æ‰¿è‡ª â†’ çº¦æŸé…ç½® ç»§æ‰¿è‡ª â†’ åŸºç¡€é…ç½®
TitaParkourCfgPPO â†’ TitaConstraintRoughCfgPPO â†’ LeggedRobotCfgPPO

æ‰€æœ‰é…ç½®éƒ½ä½¿ç”¨ NP3O âœ…
```

---

## 7ï¸âƒ£ è¯¾ç¨‹å­¦ä¹  + NP3O

### è¯¾ç¨‹å­¦ä¹ å¦‚ä½•å½±å“ NP3Oï¼Ÿ

**å…³é”®ç‚¹**ï¼šè¯¾ç¨‹å­¦ä¹ **åªè°ƒæ•´å¥–åŠ±æƒé‡**ï¼Œä¸æ”¹å˜ç®—æ³•

```python
# é˜¶æ®µ 1ï¼šå¹³åœ°è¡Œèµ°
rewards = {
    'tracking_lin_vel': 1.0,
    'obstacle_clearance': 0.5,  # ä½æƒé‡
    'collision': -0.5,          # è½»åº¦æƒ©ç½š
}
costs = {  # âœ… çº¦æŸä¿æŒä¸å˜
    'pos_limit': 0.3,
    'torque_limit': 0.3,
    ...
}

# é˜¶æ®µ 3ï¼šå®Œç¾è·‘é…·
rewards = {
    'tracking_lin_vel': 1.0,
    'obstacle_clearance': 2.0,  # é«˜æƒé‡
    'collision': -5.0,          # ä¸¥å‰æƒ©ç½š
}
costs = {  # âœ… çº¦æŸä¾ç„¶ä¸å˜
    'pos_limit': 0.3,
    'torque_limit': 0.3,
    ...
}
```

**NP3O çš„ä½œç”¨**ï¼š
- **å¥–åŠ±**ï¼šéšè¯¾ç¨‹å­¦ä¹ åŠ¨æ€è°ƒæ•´ï¼ˆé¼“åŠ±è·‘é…·æŠ€èƒ½ï¼‰
- **çº¦æŸ**ï¼šå§‹ç»ˆä¿æŒï¼ˆç¡®ä¿å®‰å…¨æ€§ï¼‰

### è¯¾ç¨‹å­¦ä¹ çš„æ›´æ–°æ–¹å¼

```python
# train_parkour.py ä¸­çš„è¯¾ç¨‹å­¦ä¹ 
for iteration in range(max_iterations):
    # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ‡æ¢é˜¶æ®µ
    if iteration == 10000:  # é˜¶æ®µ 1 â†’ é˜¶æ®µ 2
        # âœ… åªæ›´æ–°å¥–åŠ±æƒé‡
        env.cfg.rewards.scales.obstacle_clearance = 1.0  # ä» 0.5 å¢åŠ 
        env.cfg.rewards.scales.jump_timing = 0.5        # æ–°å¢
        
        # âŒ ä¸æ”¹å˜çº¦æŸ
        # env.cfg.costs.scales ä¿æŒä¸å˜
    
    # NP3O æ­£å¸¸è®­ç»ƒï¼ˆåŒæ—¶ä¼˜åŒ–å¥–åŠ±å’Œçº¦æŸï¼‰
    ppo_runner.update()
```

---

## 8ï¸âƒ£ NP3O å®ç°ç»†èŠ‚

### ä»£ç ä½ç½®

```bash
tita_rl/
â”œâ”€â”€ algorithm/
â”‚   â”œâ”€â”€ np3o.py              # âœ… NP3O æ ¸å¿ƒå®ç°
â”‚   â””â”€â”€ ppo.py               # æ ‡å‡† PPOï¼ˆå¯¹æ¯”å‚è€ƒï¼‰
â”œâ”€â”€ runner/
â”‚   â””â”€â”€ on_constraint_policy_runner.py  # âœ… çº¦æŸç­–ç•¥è¿è¡Œå™¨
â”œâ”€â”€ modules/
â”‚   â””â”€â”€ actor_critic.py      # âœ… Actor-Critic ç½‘ç»œï¼ˆå« Cost Criticï¼‰
â””â”€â”€ configs/
    â””â”€â”€ tita_parkour_config.py  # é…ç½® NP3O å‚æ•°
```

### å…³é”®ä»£ç ç‰‡æ®µ

```python
# algorithm/np3o.py

class NP3O:
    def __init__(self, cost_value_loss_coef=1.0, cost_viol_loss_coef=1.0):
        self.cost_value_loss_coef = cost_value_loss_coef
        self.cost_viol_loss_coef = cost_viol_loss_coef
        
        # ä¼˜åŒ–å™¨ï¼ˆåŒæ—¶ä¼˜åŒ– Actorã€Criticã€Cost Criticï¼‰
        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=learning_rate)
    
    def process_env_step(self, rewards, costs, dones, infos):
        """å¤„ç†ç¯å¢ƒåé¦ˆï¼ˆå¥–åŠ± + ä»£ä»·ï¼‰"""
        self.transition.rewards = rewards.clone()
        self.transition.costs = costs.clone()  # âœ… å­˜å‚¨ä»£ä»·
        self.transition.dones = dones
        self.storage.add_transitions(self.transition)
    
    def update(self):
        """æ›´æ–°ç­–ç•¥ï¼ˆNP3O ä¼˜åŒ–ï¼‰"""
        # è®¡ç®—å¥–åŠ±ä¼˜åŠ¿
        reward_advantages = self.compute_advantages(self.storage.rewards)
        
        # âœ… è®¡ç®—ä»£ä»·ä¼˜åŠ¿
        cost_advantages = self.compute_advantages(self.storage.costs)
        
        # ç­–ç•¥æŸå¤±ï¼ˆåŸºäºå¥–åŠ±ä¼˜åŠ¿ï¼‰
        policy_loss = self.compute_policy_loss(reward_advantages)
        
        # ä»·å€¼æŸå¤±ï¼ˆå¥–åŠ± Criticï¼‰
        value_loss = self.compute_value_loss()
        
        # âœ… ä»£ä»·ä»·å€¼æŸå¤±ï¼ˆCost Criticï¼‰
        cost_value_loss = self.compute_cost_value_loss()
        
        # âœ… ä»£ä»·è¿åæŸå¤±
        cost_viol_loss = self.compute_cost_violation_loss()
        
        # æ€»æŸå¤±
        total_loss = (
            policy_loss + 
            value_loss + 
            self.cost_value_loss_coef * cost_value_loss +
            self.cost_viol_loss_coef * cost_viol_loss
        )
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        total_loss.backward()
        self.optimizer.step()
```

---

## 9ï¸âƒ£ æ€»ç»“

### âœ… å…³é”®ç»“è®º

1. **è·‘é…·è®­ç»ƒä½¿ç”¨ NP3O**ï¼šå®Œå…¨ç»§æ‰¿è‡ª `tita_constraint`
2. **ç®—æ³•æ²¡æœ‰æ”¹å˜**ï¼šåªæ˜¯è°ƒæ•´äº†å¥–åŠ±æƒé‡ï¼ˆè¯¾ç¨‹å­¦ä¹ ï¼‰
3. **çº¦æŸå§‹ç»ˆå­˜åœ¨**ï¼šç¡®ä¿æœºå™¨äººå®‰å…¨è¿åŠ¨
4. **å¾®è°ƒä¹Ÿç”¨ NP3O**ï¼šåŠ è½½é¢„è®­ç»ƒæ¨¡å‹åç»§ç»­ç”¨ NP3O ä¼˜åŒ–

### ğŸ“Š é…ç½®ç¡®è®¤

```python
# configs/tita_parkour_config.py

class runner:
    algorithm_class_name = 'NP3O'  # âœ… ç¡®è®¤ä½¿ç”¨ NP3O
    runner_class_name = 'OnConstraintPolicyRunner'  # çº¦æŸè¿è¡Œå™¨
    policy_class_name = 'ActorCriticBarlowTwins'   # ç­–ç•¥ç½‘ç»œ

class algorithm:
    cost_value_loss_coef = 0.1  # âœ… NP3O ç‰¹æœ‰å‚æ•°
    cost_viol_loss_coef = 0.1   # âœ… NP3O ç‰¹æœ‰å‚æ•°

class policy:
    num_costs = 6  # âœ… 6 ä¸ªçº¦æŸ
```

### ğŸ¯ ä¸ºä»€ä¹ˆé€‰æ‹© NP3Oï¼Ÿ

| åŸå›  | è¯´æ˜ |
|------|------|
| **å®‰å…¨æ€§** | è·‘é…·åŠ¨ä½œæ¿€çƒˆï¼Œéœ€è¦çº¦æŸä¿æŠ¤ç¡¬ä»¶ |
| **å¤šç›®æ ‡** | åŒæ—¶ä¼˜åŒ–æ€§èƒ½ï¼ˆå¥–åŠ±ï¼‰å’Œå®‰å…¨ï¼ˆçº¦æŸï¼‰ |
| **Sim2Real** | çº¦æŸé™åˆ¶ä½¿ç­–ç•¥æ›´æ¥è¿‘çœŸå®ç‰©ç† |
| **ç»§æ‰¿æ€§** | åŸå§‹è®­ç»ƒå°±ç”¨ NP3Oï¼Œä¿æŒä¸€è‡´ |

### ğŸ”„ å®Œæ•´è®­ç»ƒæµç¨‹

```
1. åˆå§‹åŒ– NP3O ç®—æ³•
   â”œâ”€ Actorï¼ˆç­–ç•¥ç½‘ç»œï¼‰
   â”œâ”€ Criticï¼ˆä»·å€¼ç½‘ç»œï¼‰
   â””â”€ Cost Criticï¼ˆä»£ä»·ç½‘ç»œï¼‰âœ…

2. è¯¾ç¨‹å­¦ä¹ é˜¶æ®µ 1
   â”œâ”€ å¥–åŠ±ï¼šåŸºç¡€è¡Œèµ°æƒé‡
   â””â”€ çº¦æŸï¼šå§‹ç»ˆä¿æŒ âœ…

3. è¯¾ç¨‹å­¦ä¹ é˜¶æ®µ 2
   â”œâ”€ å¥–åŠ±ï¼šå¢åŠ è·³è·ƒæƒé‡
   â””â”€ çº¦æŸï¼šå§‹ç»ˆä¿æŒ âœ…

4. è¯¾ç¨‹å­¦ä¹ é˜¶æ®µ 3
   â”œâ”€ å¥–åŠ±ï¼šå®Œæ•´è·‘é…·æƒé‡
   â””â”€ çº¦æŸï¼šå§‹ç»ˆä¿æŒ âœ…

5. æ¯æ¬¡è¿­ä»£
   â””â”€ NP3O ä¼˜åŒ–ï¼ˆå¥–åŠ±æœ€å¤§åŒ– + çº¦æŸæ»¡è¶³ï¼‰âœ…
```

---

**æœ€ç»ˆç­”æ¡ˆ**ï¼šæ˜¯çš„ï¼Œä¾æ—§ä½¿ç”¨ **NP3O ç®—æ³•**ï¼è¿™æ˜¯ä¸€ä¸ªå¸¦çº¦æŸçš„ PPO å˜ä½“ï¼Œéå¸¸é€‚åˆè·‘é…·è¿™ç§éœ€è¦é«˜æ€§èƒ½ + é«˜å®‰å…¨æ€§çš„æœºå™¨äººæ§åˆ¶ä»»åŠ¡ã€‚è¯¾ç¨‹å­¦ä¹ åªæ˜¯è°ƒæ•´å¥–åŠ±æƒé‡ï¼Œä¸æ”¹å˜åº•å±‚çš„ NP3O ç®—æ³•ã€‚ğŸ‰
