"""
TITA è·‘é…·è®­ç»ƒè„šæœ¬ï¼ˆå¸¦è¯¾ç¨‹å­¦ä¹ ï¼‰

ä½¿ç”¨æ–¹æ³•ï¼š
python train_parkour.py --task=tita_parkour --headless

é€‰é¡¹ï¼š
--task=tita_parkour      # ä»»åŠ¡åç§°
--headless               # æ— å¤´æ¨¡å¼ï¼ˆæ— å¯è§†åŒ–ï¼‰
--resume                 # ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
--load_run=<run_name>    # æŒ‡å®šè¦åŠ è½½çš„è¿è¡Œåç§°
"""

import numpy as np
import os
from datetime import datetime

# ========== é‡è¦ï¼šIsaac Gym å¿…é¡»åœ¨ PyTorch ä¹‹å‰å¯¼å…¥ ==========
from global_config import ROOT_DIR
import isaacgym
from utils.helpers import get_args
from utils.task_registry import task_registry

# ç°åœ¨å¯ä»¥å®‰å…¨å¯¼å…¥ PyTorch ç›¸å…³æ¨¡å—
import torch

# å¯¼å…¥é…ç½®
from configs.tita_parkour_config import TitaParkourCfg, TitaParkourCfgPPO

# å¯¼å…¥ç¯å¢ƒ
from envs.parkour_robot import ParkourRobot

# å¯¼å…¥å·¥å…·
from utils.parkour_curriculum import ParkourCurriculum


def train_with_curriculum(args):
    """
    ä½¿ç”¨è¯¾ç¨‹å­¦ä¹ è®­ç»ƒè·‘é…·ç­–ç•¥
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
    """
    print("\n" + "="*70)
    print("ğŸ¯ TITA è·‘é…·è®­ç»ƒï¼ˆå¸¦è¯¾ç¨‹å­¦ä¹ ï¼‰")
    print("="*70)
    
    # åˆ›å»ºç¯å¢ƒå’Œç®—æ³•
    print("\nğŸ“¦ åˆ›å»ºè®­ç»ƒç¯å¢ƒ...")
    env, env_cfg = task_registry.make_env(name=args.task, args=args)
    
    print("ğŸ§  åˆ›å»ºç­–ç•¥ç½‘ç»œ...")
    ppo_runner, train_cfg = task_registry.make_alg_runner(env=env, name=args.task, args=args)
    
    # åˆ›å»ºè¯¾ç¨‹å­¦ä¹ ç®¡ç†å™¨
    print("\nğŸ“š åˆå§‹åŒ–è¯¾ç¨‹å­¦ä¹ ç®¡ç†å™¨...")
    curriculum = ParkourCurriculum()
    curriculum.print_curriculum_summary()
    
    # ä¿å­˜é…ç½®
    logs_path = os.path.join(ROOT_DIR, "logs")
    task_config_folder = os.path.join(logs_path, f"{args.task}")
    
    if os.path.exists(task_config_folder) and os.path.isdir(task_config_folder):
        print(f"ğŸ’¾ ä¿å­˜é…ç½®æ–‡ä»¶åˆ°: {task_config_folder}")
        task_registry.save_cfgs(name=args.task, train_cfg=train_cfg)
    else:
        print(f"âš ï¸  ä»»åŠ¡é…ç½®æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {task_config_folder}")
    
    # ============ ä¿®æ”¹ PPO Runner ä»¥æ”¯æŒè¯¾ç¨‹å­¦ä¹  ============
    original_learn = ppo_runner.learn
    
    def learn_with_curriculum(num_learning_iterations, init_at_random_ep_len=True):
        """
        é‡å†™ learn æ–¹æ³•ï¼Œæ·»åŠ è¯¾ç¨‹å­¦ä¹ é€»è¾‘
        """
        # åˆå§‹åŒ– TensorBoard writerï¼ˆå¯¹ç…§åŸå§‹ä»£ç ï¼‰
        if ppo_runner.log_dir is not None and ppo_runner.writer is None:
            from tensorboardX import SummaryWriter
            ppo_runner.writer = SummaryWriter(log_dir=ppo_runner.log_dir, flush_secs=10)
        
        # åˆå§‹åŒ–éšæœºå›åˆé•¿åº¦ï¼ˆå¯¹ç…§åŸå§‹ä»£ç ï¼‰
        if init_at_random_ep_len:
            env.episode_length_buf = torch.randint_like(env.episode_length_buf,
                                                       high=int(env.max_episode_length))
        
        print("\n" + "="*70)
        print("ğŸš€ å¼€å§‹è®­ç»ƒ")
        print("="*70)
        print(f"æ€»è¿­ä»£æ¬¡æ•°: {num_learning_iterations}")
        print(f"è¯¾ç¨‹å­¦ä¹ : å¯ç”¨")
        print("="*70 + "\n")
        
        # åˆå§‹åŒ–è§‚æµ‹
        obs = env.get_observations()
        privileged_obs = env.get_privileged_observations()
        critic_obs = privileged_obs if privileged_obs is not None else obs
        obs, critic_obs = obs.to(ppo_runner.device), critic_obs.to(ppo_runner.device)
        
        # åˆå§‹åŒ– infosï¼ˆå¯¹ç…§ on_constraint_policy_runner.pyï¼‰
        infos = {}
        if_depth = hasattr(ppo_runner, 'if_depth') and ppo_runner.if_depth
        infos["depth"] = env.depth_buffer.clone().to(ppo_runner.device) if if_depth else None
        
        ppo_runner.alg.actor_critic.train()
        
        # ä½¿ç”¨ deque è€Œä¸æ˜¯ listï¼ˆå¯¹ç…§åŸå§‹ä»£ç ï¼‰
        from collections import deque
        ep_infos = []
        rewbuffer = deque(maxlen=100)
        lenbuffer = deque(maxlen=100)
        cur_reward_sum = torch.zeros(env.num_envs, dtype=torch.float, device=ppo_runner.device)
        cur_episode_length = torch.zeros(env.num_envs, dtype=torch.float, device=ppo_runner.device)
        
        tot_iter = 0
        
        # è·å–åˆå§‹é˜¶æ®µ
        current_stage = curriculum.get_stage(0)
        curriculum.update_reward_scales(env, current_stage)
        last_update_iter = 0
        
        # è®­ç»ƒå¾ªç¯
        for it in range(num_learning_iterations):
            tot_iter += 1
            
            # ============ è¯¾ç¨‹å­¦ä¹ ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦åˆ‡æ¢é˜¶æ®µ ============
            if curriculum.should_update_config(it, update_interval=100):
                stage = curriculum.get_stage(it)
                
                # å¦‚æœè¿›å…¥æ–°é˜¶æ®µï¼Œæ›´æ–°é…ç½®
                if stage != current_stage:
                    current_stage = stage
                    print(f"\n{'='*70}")
                    print(f"ğŸ“Š è¿­ä»£ {it}: åˆ‡æ¢åˆ°æ–°é˜¶æ®µ")
                    print(f"{'='*70}")
                    curriculum.update_env_config(env, stage)
                    curriculum.update_reward_scales(env, stage)
                    last_update_iter = it
            
            # ============ æ­£å¸¸è®­ç»ƒæ­¥éª¤ ============
            start = time.time()
            
            # Rollout
            with torch.inference_mode():
                for i in range(train_cfg.runner.num_steps_per_env):
                    actions = ppo_runner.alg.act(obs, critic_obs, infos)
                    obs, privileged_obs, rewards, costs, dones, infos = env.step(actions)
                    critic_obs = privileged_obs if privileged_obs is not None else obs
                    obs, critic_obs, rewards, costs, dones = obs.to(ppo_runner.device), critic_obs.to(ppo_runner.device), rewards.to(ppo_runner.device), costs.to(ppo_runner.device), dones.to(ppo_runner.device)
                    ppo_runner.alg.process_env_step(rewards, costs, dones, infos)
                    
                    # ç»Ÿè®¡
                    if 'episode' in infos:
                        ep_infos.append(infos['episode'])
                    cur_reward_sum += rewards
                    cur_episode_length += 1
                    
                    new_ids = (dones > 0).nonzero(as_tuple=False)
                    rewbuffer.extend(cur_reward_sum[new_ids][:, 0].cpu().numpy().tolist())
                    lenbuffer.extend(cur_episode_length[new_ids][:, 0].cpu().numpy().tolist())
                    cur_reward_sum[new_ids] = 0
                    cur_episode_length[new_ids] = 0
                
                stop = time.time()
                collection_time = stop - start
                
                # è®¡ç®—å€¼å‡½æ•°ï¼ˆå¯¹ç…§åŸå§‹ä»£ç ï¼šéœ€è¦åŒæ—¶è®¡ç®— returns å’Œ cost_returnsï¼‰
                start = stop
                ppo_runner.alg.compute_returns(critic_obs)
                ppo_runner.alg.compute_cost_returns(critic_obs)
            
            # æ›´æ–° k å€¼ï¼ˆå¯¹ç…§åŸå§‹ä»£ç ï¼šç”¨äºçº¦æŸä¼˜åŒ–ï¼‰
            k_value = ppo_runner.alg.update_k_value(it)
            
            # å­¦ä¹ 
            mean_value_loss, mean_cost_value_loss, mean_viol_loss, mean_surrogate_loss, mean_imitation_loss = ppo_runner.alg.update()
            stop = time.time()
            learn_time = stop - start
            
            # ============ æ—¥å¿—è®°å½• ============
            if it % train_cfg.runner.log_interval == 0:
                # è·å–è¿›åº¦ä¿¡æ¯
                progress = curriculum.get_progress(it)
                
                # æ‰“å°åŸºæœ¬ä¿¡æ¯
                print(f"\n{'='*70}")
                print(f"ğŸ“Š è¿­ä»£ {it}/{num_learning_iterations}")
                print(f"{'='*70}")
                print(f"é˜¶æ®µ: {progress['stage_name']}")
                print(f"é˜¶æ®µè¿›åº¦: {progress['stage_progress']*100:.1f}%")
                print(f"æ€»è¿›åº¦: {progress['total_progress']*100:.1f}%")
                
                if len(rewbuffer) > 0:
                    print(f"å¹³å‡å¥–åŠ±: {np.mean(rewbuffer):.2f}")
                    print(f"å¹³å‡å›åˆé•¿åº¦: {np.mean(lenbuffer):.1f}")
                
                print(f"é‡‡é›†æ—¶é—´: {collection_time:.3f}s")
                print(f"å­¦ä¹ æ—¶é—´: {learn_time:.3f}s")
                print(f"{'='*70}\n")
                
                # è®°å½•åˆ° TensorBoardï¼ˆå¦‚æœæœ‰ï¼‰
                if hasattr(ppo_runner, 'writer') and ppo_runner.writer is not None:
                    ppo_runner.writer.add_scalar('Curriculum/stage_index', progress['stage_index'], it)
                    ppo_runner.writer.add_scalar('Curriculum/stage_progress', progress['stage_progress'], it)
                    ppo_runner.writer.add_scalar('Curriculum/total_progress', progress['total_progress'], it)
                    
                    if len(rewbuffer) > 0:
                        ppo_runner.writer.add_scalar('Train/mean_reward', np.mean(rewbuffer), it)
                        ppo_runner.writer.add_scalar('Train/mean_episode_length', np.mean(lenbuffer), it)
                
                rewbuffer.clear()
                lenbuffer.clear()
            
            # ============ ä¿å­˜æ£€æŸ¥ç‚¹ ============
            if it % train_cfg.runner.save_interval == 0:
                print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹ (iteration {it})...")
                ppo_runner.save(os.path.join(ppo_runner.log_dir, f'model_{it}.pt'))
        
        # è®­ç»ƒç»“æŸï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹
        print(f"\n{'='*70}")
        print("âœ… è®­ç»ƒå®Œæˆï¼")
        print(f"{'='*70}")
        print(f"ğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
        ppo_runner.save(os.path.join(ppo_runner.log_dir, 'model_final.pt'))
        print(f"æ¨¡å‹ä¿å­˜ä½ç½®: {ppo_runner.log_dir}")
        print(f"{'='*70}\n")
    
    # æ›¿æ¢ learn æ–¹æ³•
    import time
    ppo_runner.learn_with_curriculum = learn_with_curriculum
    
    # å¼€å§‹è®­ç»ƒ
    print("\nğŸ“ ä½¿ç”¨è¯¾ç¨‹å­¦ä¹ è®­ç»ƒç­–ç•¥...")
    print(f"   - é˜¶æ®µ1: {curriculum.stages[0]['iterations']} iterations")
    print(f"   - é˜¶æ®µ2: {curriculum.stages[1]['iterations']} iterations")
    print(f"   - é˜¶æ®µ3: {curriculum.stages[2]['iterations']} iterations")
    print(f"   - æ€»è®¡: {curriculum.get_total_iterations()} iterations\n")
    
    ppo_runner.learn_with_curriculum(
        num_learning_iterations=train_cfg.runner.max_iterations,
        init_at_random_ep_len=True
    )


def main():
    """ä¸»å‡½æ•°"""
    # æ³¨å†Œä»»åŠ¡
    print("ğŸ“ æ³¨å†Œè·‘é…·ä»»åŠ¡...")
    task_registry.register(
        "tita_parkour",
        ParkourRobot,
        TitaParkourCfg(),
        TitaParkourCfgPPO()
    )
    
    # è·å–å‚æ•°
    args = get_args()
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šä»»åŠ¡ï¼Œé»˜è®¤ä½¿ç”¨ tita_parkour
    if not hasattr(args, 'task') or args.task is None:
        args.task = 'tita_parkour'
        print(f"âš™ï¸  ä½¿ç”¨é»˜è®¤ä»»åŠ¡: {args.task}")
    
    # å¼€å§‹è®­ç»ƒ
    train_with_curriculum(args)


if __name__ == '__main__':
    main()
