"""
TITA è·‘é…·ç­–ç•¥æµ‹è¯•å’Œè¯„ä¼°è„šæœ¬

ä½¿ç”¨æ–¹æ³•ï¼š
python test_parkour.py --load_run=<run_name> --checkpoint=<checkpoint_name>

ç¤ºä¾‹ï¼š
python test_parkour.py --load_run=parkour_with_curriculum --checkpoint=model_final.pt
python test_parkour.py --load_run=parkour_with_curriculum --checkpoint=model_37000.pt --num_envs=100
"""

import numpy as np
import os
import torch
from datetime import datetime

# å¯¼å…¥é…ç½®
from configs.tita_parkour_config import TitaParkourCfg, TitaParkourCfgPPO

# å¯¼å…¥ç¯å¢ƒ
from envs.parkour_robot import ParkourRobot

# å¯¼å…¥å·¥å…·
from global_config import ROOT_DIR
import isaacgym
from utils.helpers import get_args
from utils.task_registry import task_registry


class ParkourEvaluator:
    """è·‘é…·ç­–ç•¥è¯„ä¼°å™¨"""
    
    def __init__(self, env, policy_path, device='cuda:0'):
        """
        Args:
            env: ç¯å¢ƒå®ä¾‹
            policy_path: ç­–ç•¥æ–‡ä»¶è·¯å¾„
            device: è®¾å¤‡
        """
        self.env = env
        self.device = device
        
        # åŠ è½½ç­–ç•¥
        print(f"ğŸ“¦ åŠ è½½ç­–ç•¥: {policy_path}")
        self.policy = torch.jit.load(policy_path).to(device)
        self.policy.eval()
        
        # ç»Ÿè®¡ç¼“å†²åŒº
        self.reset_statistics()
    
    def reset_statistics(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.episode_rewards = []
        self.episode_lengths = []
        self.obstacle_success_count = 0
        self.jump_count = 0
        self.fall_count = 0
        self.collision_count = 0
        
        # å½“å‰å›åˆç»Ÿè®¡
        self.current_reward = torch.zeros(self.env.num_envs, device=self.device)
        self.current_length = torch.zeros(self.env.num_envs, device=self.device, dtype=torch.int)
    
    def evaluate(self, num_episodes=100, max_steps=1000):
        """
        è¯„ä¼°ç­–ç•¥
        
        Args:
            num_episodes: è¯„ä¼°å›åˆæ•°
            max_steps: æ¯ä¸ªå›åˆæœ€å¤§æ­¥æ•°
            
        Returns:
            dict: è¯„ä¼°ç»“æœ
        """
        print(f"\n{'='*70}")
        print(f"ğŸ¯ å¼€å§‹è¯„ä¼°")
        print(f"{'='*70}")
        print(f"è¯„ä¼°å›åˆæ•°: {num_episodes}")
        print(f"æ¯å›åˆæœ€å¤§æ­¥æ•°: {max_steps}")
        print(f"{'='*70}\n")
        
        self.reset_statistics()
        obs = self.env.reset()
        
        completed_episodes = 0
        step = 0
        
        with torch.no_grad():
            while completed_episodes < num_episodes and step < max_steps * num_episodes:
                # æ‰§è¡ŒåŠ¨ä½œ
                actions = self.policy(obs)
                obs, _, rewards, _, dones, infos = self.env.step(actions)
                
                # æ›´æ–°ç»Ÿè®¡
                self.current_reward += rewards
                self.current_length += 1
                
                # æ£€æŸ¥å®Œæˆçš„å›åˆ
                done_ids = dones.nonzero(as_tuple=False).flatten()
                if len(done_ids) > 0:
                    for env_id in done_ids:
                        self.episode_rewards.append(self.current_reward[env_id].item())
                        self.episode_lengths.append(self.current_length[env_id].item())
                        
                        # é‡ç½®è¯¥ç¯å¢ƒçš„ç»Ÿè®¡
                        self.current_reward[env_id] = 0
                        self.current_length[env_id] = 0
                        
                        completed_episodes += 1
                        
                        # è¿›åº¦æ˜¾ç¤º
                        if completed_episodes % 10 == 0:
                            print(f"  å®Œæˆ {completed_episodes}/{num_episodes} å›åˆ...")
                
                step += 1
        
        # è®¡ç®—ç»Ÿè®¡ç»“æœ
        results = self._compute_statistics()
        
        return results
    
    def _compute_statistics(self):
        """è®¡ç®—è¯„ä¼°ç»Ÿè®¡"""
        results = {
            'mean_reward': np.mean(self.episode_rewards) if self.episode_rewards else 0,
            'std_reward': np.std(self.episode_rewards) if self.episode_rewards else 0,
            'min_reward': np.min(self.episode_rewards) if self.episode_rewards else 0,
            'max_reward': np.max(self.episode_rewards) if self.episode_rewards else 0,
            
            'mean_length': np.mean(self.episode_lengths) if self.episode_lengths else 0,
            'std_length': np.std(self.episode_lengths) if self.episode_lengths else 0,
            
            'num_episodes': len(self.episode_rewards),
        }
        
        return results
    
    def print_results(self, results):
        """æ‰“å°è¯„ä¼°ç»“æœ"""
        print(f"\n{'='*70}")
        print("ğŸ“Š è¯„ä¼°ç»“æœ")
        print(f"{'='*70}")
        print(f"è¯„ä¼°å›åˆæ•°: {results['num_episodes']}")
        print(f"\nå¥–åŠ±ç»Ÿè®¡:")
        print(f"  å¹³å‡å¥–åŠ±: {results['mean_reward']:.2f} Â± {results['std_reward']:.2f}")
        print(f"  æœ€å°å¥–åŠ±: {results['min_reward']:.2f}")
        print(f"  æœ€å¤§å¥–åŠ±: {results['max_reward']:.2f}")
        print(f"\nå›åˆé•¿åº¦:")
        print(f"  å¹³å‡é•¿åº¦: {results['mean_length']:.1f} Â± {results['std_length']:.1f} steps")
        print(f"{'='*70}\n")


def test_parkour(args):
    """
    æµ‹è¯•è·‘é…·ç­–ç•¥
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
    """
    print("\n" + "="*70)
    print("ğŸ¯ TITA è·‘é…·ç­–ç•¥æµ‹è¯•")
    print("="*70)
    
    # åˆ›å»ºç¯å¢ƒ
    print("\nğŸ“¦ åˆ›å»ºæµ‹è¯•ç¯å¢ƒ...")
    env_cfg = TitaParkourCfg()
    env_cfg.env.num_envs = min(args.num_envs, 100) if hasattr(args, 'num_envs') else 100
    
    from utils import class_to_dict
    from isaacgym import gymutil
    
    # åˆ›å»ºä»¿çœŸå‚æ•°
    sim_params = gymutil.parse_sim_config(vars(env_cfg.sim))
    
    # åˆ›å»ºç¯å¢ƒ
    env = ParkourRobot(
        cfg=env_cfg,
        sim_params=sim_params,
        physics_engine=args.physics_engine,
        sim_device=args.sim_device,
        headless=args.headless
    )
    
    # æ„å»ºç­–ç•¥è·¯å¾„
    if hasattr(args, 'load_run') and args.load_run:
        log_root = os.path.join(ROOT_DIR, 'logs', args.task)
        log_dir = os.path.join(log_root, args.load_run)
        
        if hasattr(args, 'checkpoint') and args.checkpoint:
            policy_path = os.path.join(log_dir, args.checkpoint)
        else:
            policy_path = os.path.join(log_dir, 'model_final.pt')
    else:
        raise ValueError("è¯·æŒ‡å®š --load_run å‚æ•°")
    
    if not os.path.exists(policy_path):
        raise FileNotFoundError(f"ç­–ç•¥æ–‡ä»¶ä¸å­˜åœ¨: {policy_path}")
    
    print(f"ç­–ç•¥æ–‡ä»¶: {policy_path}")
    print(f"æµ‹è¯•ç¯å¢ƒæ•°: {env.num_envs}")
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = ParkourEvaluator(
        env=env,
        policy_path=policy_path,
        device=args.sim_device
    )
    
    # è¿è¡Œè¯„ä¼°
    num_episodes = args.num_test_episodes if hasattr(args, 'num_test_episodes') else 100
    results = evaluator.evaluate(num_episodes=num_episodes)
    
    # æ‰“å°ç»“æœ
    evaluator.print_results(results)
    
    # ä¿å­˜ç»“æœ
    if hasattr(args, 'save_results') and args.save_results:
        results_path = os.path.join(log_dir, f'test_results_{datetime.now().strftime("%Y%m%d_%H%M%S")}.txt')
        with open(results_path, 'w') as f:
            f.write("TITA è·‘é…·è¯„ä¼°ç»“æœ\n")
            f.write("="*50 + "\n\n")
            for key, value in results.items():
                f.write(f"{key}: {value}\n")
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {results_path}")


def main():
    """ä¸»å‡½æ•°"""
    # æ³¨å†Œä»»åŠ¡
    print("ğŸ“ æ³¨å†Œè·‘é…·ä»»åŠ¡...")
    task_registry.register(
        "tita_parkour",
        ParkourRobot,
        TitaParkourCfg(),
        TitaParkourCfgPPO()
    )
    
    # è·å–å‚æ•°
    args = get_args()
    
    # è®¾ç½®é»˜è®¤å‚æ•°
    if not hasattr(args, 'task') or args.task is None:
        args.task = 'tita_parkour'
    
    if not hasattr(args, 'num_envs'):
        args.num_envs = 100
    
    if not hasattr(args, 'num_test_episodes'):
        args.num_test_episodes = 100
    
    if not hasattr(args, 'headless'):
        args.headless = False  # æµ‹è¯•æ—¶é»˜è®¤æ˜¾ç¤ºå¯è§†åŒ–
    
    # å¼€å§‹æµ‹è¯•
    test_parkour(args)


if __name__ == '__main__':
    main()
